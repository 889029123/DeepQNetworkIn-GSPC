{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fd6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Collecting colorama>=0.4.6\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in d:\\anaconda\\lib\\site-packages (from bayesian-optimization) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in d:\\anaconda\\lib\\site-packages (from bayesian-optimization) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in d:\\anaconda\\lib\\site-packages (from bayesian-optimization) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
      "Installing collected packages: colorama, bayesian-optimization\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7262634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "class Q_Network(chainer.Chain):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Q_Network, self).__init__(\n",
    "            fc1 = L.Linear(input_size, hidden_size),\n",
    "            fc2 = L.Linear(hidden_size, hidden_size),\n",
    "            fc3 = L.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "    def reset(self):\n",
    "        self.zerograds()\n",
    "\n",
    "class Environment1:\n",
    "    def __init__(self, data, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 10000\n",
    "        self.count = []\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.brok_rate = 0.0009\n",
    "        self.max_trade_percent = 0.8\n",
    "        self.tbrokerage = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        self.Act0 = 0\n",
    "        self.Act1 = 0\n",
    "        self.Act2 = 0\n",
    "        self.RW_p = 0\n",
    "        self.RW_n = 0\n",
    "        self.RW_p_v = 0\n",
    "        self.RW_n_v = 0\n",
    "        return [self.position_value] + self.history\n",
    "    \n",
    "    def step(self, act, amount):\n",
    "        if self.t >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "            return [self.position_value] + self.history, 0, self.done\n",
    "\n",
    "        reward = 0\n",
    "        if act == 1:\n",
    "            if self.profits != 0:\n",
    "                max_trade_amount = self.profits * self.max_trade_percent\n",
    "                stock_price = self.data.iloc[self.t, :]['Close']\n",
    "                count = max_trade_amount / stock_price\n",
    "                buyin = stock_price * count\n",
    "                self.profits -= buyin\n",
    "                self.positions.append(stock_price)\n",
    "                self.count.append(count)\n",
    "                self.Act1 += 1\n",
    "        elif act == 2:\n",
    "            if len(self.positions) > 0:\n",
    "                sell_ratio = self.determine_sell_ratio(amount)\n",
    "                num_positions_to_sell = int(len(self.positions) * sell_ratio)\n",
    "                for i in range(num_positions_to_sell):\n",
    "                    sell_price = self.data.iloc[self.t, :]['Close']\n",
    "                    buy_price = self.positions[i]\n",
    "                    count = self.count[i]\n",
    "                    abs_num = (sell_price - buy_price) * count\n",
    "                    if abs_num > 0:\n",
    "                        self.RW_p += 1\n",
    "                        self.RW_p_v += abs_num\n",
    "                    else:\n",
    "                        self.RW_n += 1\n",
    "                        self.RW_n_v += abs(abs_num)\n",
    "                    reward += sell_price * count\n",
    "                    self.profits += sell_price * count\n",
    "                self.positions = self.positions[num_positions_to_sell:]\n",
    "                self.count = self.count[num_positions_to_sell:]\n",
    "                self.Act2 += 1    \n",
    "        else:\n",
    "            self.Act0 += 1\n",
    "        \n",
    "        self.t += 1\n",
    "        if self.t >= len(self.data):\n",
    "            self.done = True\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['Close'] - p)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n",
    "        \n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward < 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history, reward, self.done\n",
    "    \n",
    "    def determine_sell_ratio(self, signal_strength):\n",
    "        thresholds = [0.2, 0.5, 0.8]\n",
    "        ratios = [0.25, 0.5, 0.75]\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if signal_strength < threshold:\n",
    "                return ratios[i]\n",
    "        return ratios[0]\n",
    "\n",
    "def train_dqn(env, Q, epoch_num=1):\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 1\n",
    "    confidence_threshold_buy = 0.1\n",
    "    confidence_threshold_sell = 0.8\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        RandAct = 0\n",
    "        NRandAct = 0\n",
    "        while not done and step < step_max:\n",
    "            pact = np.random.randint(3)\n",
    "            amount = 0.25\n",
    "            if np.random.rand() > epsilon:\n",
    "                q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                softmax_q_values = F.softmax(q_values).data\n",
    "                amount = softmax_q_values.ravel()[pact]\n",
    "                pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "                if pact == 2:\n",
    "                    if amount < confidence_threshold_sell:\n",
    "                        pact = 0\n",
    "                if pact == 1:\n",
    "                    if amount < confidence_threshold_buy:\n",
    "                        pact = 0\n",
    "                NRandAct+=1\n",
    "            else:\n",
    "                RandAct+=1\n",
    "                if pact==0: amount = 0.5\n",
    "            obs, reward, done = env.step(pact,amount)\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_profits = env.profits\n",
    "            if isinstance(env.count, list) and len(env.count) != 0:\n",
    "                for i in range(len(env.positions)):\n",
    "                    log_profits += env.count[i] * env.data.iloc[env.t, :]['Close']\n",
    "            else:\n",
    "                log_profits = env.profits\n",
    "            elapsed_time = time.time()-start\n",
    "            print(f\"Training Epoch {epoch+1} - Profit: {log_profits}\")\n",
    "            start = time.time()       \n",
    "    return Q, total_losses, total_rewards\n",
    "\n",
    "def test_dqn(test_env, Q, confidence_threshold_buy=0.1, confidence_threshold_sell=0.8):\n",
    "    pobs = test_env.reset()\n",
    "    test_env.profits = 10000\n",
    "    test_acts = []\n",
    "    test_rewards = []\n",
    "    estimated_total_assets = [test_env.profits]\n",
    "\n",
    "    while not test_env.done:\n",
    "        q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        softmax_q_values = F.softmax(q_values).data\n",
    "        pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "        amount = softmax_q_values.ravel()[pact]\n",
    "        if pact == 2 and amount < confidence_threshold_sell:\n",
    "            pact = 0\n",
    "        elif pact == 1 and amount < confidence_threshold_buy:\n",
    "            pact = 0\n",
    "        obs, reward, done = test_env.step(pact, amount)\n",
    "        test_acts.append(pact)\n",
    "        test_rewards.append(reward)\n",
    "        pobs = obs\n",
    "        stock_value = sum(c * test_env.data.iloc[test_env.t]['Close'] for c in test_env.count)\n",
    "        total_assets = test_env.profits + stock_value\n",
    "        estimated_total_assets.append(total_assets)\n",
    "\n",
    "    final_profits = total_assets\n",
    "    return test_acts, test_rewards, final_profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4463981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_41364/3458813212.py:172: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  shuffled_memory = np.random.permutation(memory)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_41364/3458813212.py:180: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 - Profit: 20003.57970533339\n",
      "Training Epoch 2 - Profit: 18577.855961387984\n",
      "Training Epoch 3 - Profit: 19609.998555795682\n",
      "Training Epoch 4 - Profit: 16317.31628373102\n",
      "Training Epoch 5 - Profit: 20102.051661886133\n",
      "Training Epoch 6 - Profit: 18226.62702733439\n",
      "Training Epoch 7 - Profit: 20154.616679237843\n",
      "Training Epoch 8 - Profit: 17949.16792134716\n",
      "Training Epoch 9 - Profit: 17271.266355331943\n",
      "Training Epoch 10 - Profit: 19848.32463457443\n",
      "Training Epoch 11 - Profit: 24513.94427490572\n",
      "Training Epoch 12 - Profit: 21588.96760580288\n",
      "Training Epoch 13 - Profit: 18960.494195512147\n",
      "Training Epoch 14 - Profit: 20044.86143444289\n",
      "Training Epoch 15 - Profit: 18093.89619653825\n",
      "Training Epoch 16 - Profit: 20282.034821410994\n",
      "Training Epoch 17 - Profit: 22130.658445957986\n",
      "Training Epoch 18 - Profit: 19754.342378382422\n",
      "Training Epoch 19 - Profit: 21955.46350700025\n",
      "Training Epoch 20 - Profit: 25667.916431143363\n",
      "Training Epoch 21 - Profit: 22743.57464098319\n",
      "Training Epoch 22 - Profit: 20923.645675259533\n",
      "Training Epoch 23 - Profit: 25608.860210009247\n",
      "Training Epoch 24 - Profit: 20672.637195541403\n",
      "Training Epoch 25 - Profit: 19698.160763455224\n",
      "Training Epoch 26 - Profit: 18874.73169030759\n",
      "Training Epoch 27 - Profit: 21382.362141685833\n",
      "Training Epoch 28 - Profit: 23228.711703842186\n",
      "Training Epoch 29 - Profit: 21473.86451808413\n",
      "|   iter    |  target   | confid... | confid... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m1.105e+04\u001b[0m | \u001b[0m0.3919   \u001b[0m | \u001b[0m0.6042   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m1.163e+04\u001b[0m | \u001b[95m0.1001   \u001b[0m | \u001b[95m0.3116   \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m1.141e+04\u001b[0m | \u001b[0m0.2027   \u001b[0m | \u001b[0m0.1646   \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m1.134e+04\u001b[0m | \u001b[0m0.2304   \u001b[0m | \u001b[0m0.3419   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m1.105e+04\u001b[0m | \u001b[0m0.3777   \u001b[0m | \u001b[0m0.4772   \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m1.126e+04\u001b[0m | \u001b[0m0.3934   \u001b[0m | \u001b[0m0.5797   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m1.154e+04\u001b[0m | \u001b[0m0.2431   \u001b[0m | \u001b[0m0.7147   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m1.137e+04\u001b[0m | \u001b[0m0.1192   \u001b[0m | \u001b[0m0.5693   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m1.126e+04\u001b[0m | \u001b[0m0.3921   \u001b[0m | \u001b[0m0.4911   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m1.138e+04\u001b[0m | \u001b[0m0.1983   \u001b[0m | \u001b[0m0.2387   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m1.146e+04\u001b[0m | \u001b[0m0.1002   \u001b[0m | \u001b[0m0.3279   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m1.121e+04\u001b[0m | \u001b[0m0.2412   \u001b[0m | \u001b[0m0.7134   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m1.158e+04\u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m0.5402   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m1.151e+04\u001b[0m | \u001b[0m0.1603   \u001b[0m | \u001b[0m0.6311   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m1.126e+04\u001b[0m | \u001b[0m0.3769   \u001b[0m | \u001b[0m0.213    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.7214   \u001b[0m | \u001b[0m0.413    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m1.105e+04\u001b[0m | \u001b[0m0.4026   \u001b[0m | \u001b[0m0.3464   \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.5269   \u001b[0m | \u001b[0m0.375    \u001b[0m |\n",
      "| \u001b[95m19       \u001b[0m | \u001b[95m1.169e+04\u001b[0m | \u001b[95m0.4872   \u001b[0m | \u001b[95m0.5914   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m1.116e+04\u001b[0m | \u001b[0m0.2039   \u001b[0m | \u001b[0m0.3847   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m1.042e+04\u001b[0m | \u001b[0m0.4859   \u001b[0m | \u001b[0m0.5949   \u001b[0m |\n",
      "| \u001b[95m22       \u001b[0m | \u001b[95m1.182e+04\u001b[0m | \u001b[95m0.118    \u001b[0m | \u001b[95m0.569    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m1.063e+04\u001b[0m | \u001b[0m0.486    \u001b[0m | \u001b[0m0.5904   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.7368   \u001b[0m | \u001b[0m0.3912   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m1.158e+04\u001b[0m | \u001b[0m0.2177   \u001b[0m | \u001b[0m0.5789   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m1.109e+04\u001b[0m | \u001b[0m0.2294   \u001b[0m | \u001b[0m0.3441   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m1.155e+04\u001b[0m | \u001b[0m0.1558   \u001b[0m | \u001b[0m0.539    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m1.063e+04\u001b[0m | \u001b[0m0.3778   \u001b[0m | \u001b[0m0.2132   \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m1.137e+04\u001b[0m | \u001b[0m0.2536   \u001b[0m | \u001b[0m0.683    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m1.148e+04\u001b[0m | \u001b[0m0.4572   \u001b[0m | \u001b[0m0.645    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.6909   \u001b[0m | \u001b[0m0.5268   \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.6541   \u001b[0m | \u001b[0m0.638    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m1.134e+04\u001b[0m | \u001b[0m0.1072   \u001b[0m | \u001b[0m0.3555   \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m1.105e+04\u001b[0m | \u001b[0m0.423    \u001b[0m | \u001b[0m0.4416   \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m1.148e+04\u001b[0m | \u001b[0m0.4461   \u001b[0m | \u001b[0m0.1639   \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m1.112e+04\u001b[0m | \u001b[0m0.32     \u001b[0m | \u001b[0m0.2501   \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.7162   \u001b[0m | \u001b[0m0.1925   \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m1.129e+04\u001b[0m | \u001b[0m0.1162   \u001b[0m | \u001b[0m0.1193   \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m1.139e+04\u001b[0m | \u001b[0m0.1436   \u001b[0m | \u001b[0m0.3728   \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m1e+04    \u001b[0m | \u001b[0m0.6171   \u001b[0m | \u001b[0m0.3087   \u001b[0m |\n",
      "=================================================\n",
      "{'target': 11820.305045010839, 'params': {'confidence_threshold_buy': 0.11803540036662069, 'confidence_threshold_sell': 0.5689542195973996}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 你的 Environment1, Q_Network, train_dqn 和 test_dqn 定義\n",
    "# 省略前面的程式碼定義以便於展示貝葉斯優化的重點部分\n",
    "\n",
    "def optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019):\n",
    "    # 資料分割\n",
    "    test_start_date_2010_2019 = start_date_2010 + datetime.timedelta(days=train_days_2010_2019)\n",
    "    train_end_date_2010_2019 = test_start_date_2010_2019 - datetime.timedelta(days=1)\n",
    "\n",
    "    train = data[start_date_2010.date():train_end_date_2010_2019.date()]\n",
    "    test = data[test_start_date_2010_2019.date():end_date_2019.date()]\n",
    "    train_env = Environment1(train)\n",
    "    test_env = Environment1(test)\n",
    "    \n",
    "    # 預訓練 Q 網絡\n",
    "    Q = Q_Network(input_size=train_env.history_t + 1, hidden_size=100, output_size=3)\n",
    "    Q, total_losses, total_rewards = train_dqn(train_env, Q, epoch_num=29)\n",
    "    chainer.serializers.save_npz(f'Q_network_epoch_{29}.npz', Q)\n",
    "    \n",
    "    def objective(confidence_threshold_buy, confidence_threshold_sell):\n",
    "        avg_profits = []\n",
    "        for _ in range(10):\n",
    "            _, _, test_profits = test_dqn(test_env, Q, confidence_threshold_buy=confidence_threshold_buy, confidence_threshold_sell=confidence_threshold_sell)\n",
    "            avg_profits.append(test_profits)\n",
    "        return np.mean(avg_profits)\n",
    "    \n",
    "    pbounds = {\n",
    "        'confidence_threshold_buy': (0.1, 0.8),\n",
    "        'confidence_threshold_sell': (0.1, 0.8)\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "    \n",
    "    optimizer.maximize(\n",
    "        init_points=10,\n",
    "        n_iter=30,\n",
    "    )\n",
    "    \n",
    "    print(optimizer.max)\n",
    "    \n",
    "data = pd.read_csv('C:/Users/User/Documents/RL_/Data/Stocks/SPY_Both.txt')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "start_date_2010 = datetime.datetime(2010, 1, 1)\n",
    "end_date_2019 = datetime.datetime(2019, 12, 31)\n",
    "total_days_2010_2019 = (end_date_2019 - start_date_2010).days\n",
    "train_days_2010_2019 = int(total_days_2010_2019 * 0.9)\n",
    "test_days_2010_2019 = total_days_2010_2019 - train_days_2010_2019\n",
    "\n",
    "optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c09d04b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | epoch_num |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m1.273e+04\u001b[0m | \u001b[0m141.6    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m1.27e+04 \u001b[0m | \u001b[0m223.5    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m1.276e+04\u001b[0m | \u001b[95m29.03    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m1.28e+04 \u001b[0m | \u001b[95m110.6    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m68.62    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m1.267e+04\u001b[0m | \u001b[0m53.93    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m79.29    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m122.3    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m1.271e+04\u001b[0m | \u001b[0m136.1    \u001b[0m |\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m1.292e+04\u001b[0m | \u001b[95m174.5    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m181.1    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m172.5    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m1.255e+04\u001b[0m | \u001b[0m294.4    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m29.59    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m1.271e+04\u001b[0m | \u001b[0m175.1    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m1.276e+04\u001b[0m | \u001b[0m168.0    \u001b[0m |\n",
      "| \u001b[95m17       \u001b[0m | \u001b[95m1.292e+04\u001b[0m | \u001b[95m174.5    \u001b[0m |\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m1.299e+04\u001b[0m | \u001b[95m174.5    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m174.7    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m1.279e+04\u001b[0m | \u001b[0m110.2    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m1.269e+04\u001b[0m | \u001b[0m111.0    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m1.276e+04\u001b[0m | \u001b[0m109.9    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m1.279e+04\u001b[0m | \u001b[0m68.13    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m174.6    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m1.297e+04\u001b[0m | \u001b[0m174.8    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m1.274e+04\u001b[0m | \u001b[0m67.73    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m1.273e+04\u001b[0m | \u001b[0m64.85    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m1.266e+04\u001b[0m | \u001b[0m284.3    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m1.27e+04 \u001b[0m | \u001b[0m198.5    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m1.295e+04\u001b[0m | \u001b[0m174.9    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m1.257e+04\u001b[0m | \u001b[0m218.1    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m69.15    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m1.276e+04\u001b[0m | \u001b[0m69.68    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m1.262e+04\u001b[0m | \u001b[0m42.15    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m1.273e+04\u001b[0m | \u001b[0m243.5    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m1.263e+04\u001b[0m | \u001b[0m246.5    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m1.284e+04\u001b[0m | \u001b[0m70.26    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m1.281e+04\u001b[0m | \u001b[0m70.05    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m1.282e+04\u001b[0m | \u001b[0m70.5     \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m1.282e+04\u001b[0m | \u001b[0m70.78    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m71.04    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m1.267e+04\u001b[0m | \u001b[0m125.7    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m144.6    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m1.279e+04\u001b[0m | \u001b[0m231.1    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m231.5    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m1.269e+04\u001b[0m | \u001b[0m230.8    \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m231.9    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m1.274e+04\u001b[0m | \u001b[0m232.4    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m1.276e+04\u001b[0m | \u001b[0m280.7    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m1.268e+04\u001b[0m | \u001b[0m288.9    \u001b[0m |\n",
      "=====================================\n",
      "{'target': 12986.739606176254, 'params': {'epoch_num': 174.53851254058796}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class Q_Network(chainer.Chain):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Q_Network, self).__init__(\n",
    "            fc1 = L.Linear(input_size, hidden_size),\n",
    "            fc2 = L.Linear(hidden_size, hidden_size),\n",
    "            fc3 = L.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "    def reset(self):\n",
    "        self.zerograds()\n",
    "\n",
    "class Environment1:\n",
    "    def __init__(self, data, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 10000\n",
    "        self.count = []\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.brok_rate = 0.0009\n",
    "        self.max_trade_percent = 0.8\n",
    "        self.tbrokerage = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        self.Act0 = 0\n",
    "        self.Act1 = 0\n",
    "        self.Act2 = 0\n",
    "        self.RW_p = 0\n",
    "        self.RW_n = 0\n",
    "        self.RW_p_v = 0\n",
    "        self.RW_n_v = 0\n",
    "        return [self.position_value] + self.history\n",
    "    \n",
    "    def step(self, act, amount):\n",
    "        if self.t >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "            return [self.position_value] + self.history, 0, self.done\n",
    "\n",
    "        reward = 0\n",
    "        if act == 1:\n",
    "            if self.profits != 0:\n",
    "                max_trade_amount = self.profits * self.max_trade_percent\n",
    "                stock_price = self.data.iloc[self.t, :]['Close']\n",
    "                count = max_trade_amount / stock_price\n",
    "                buyin = stock_price * count\n",
    "                self.profits -= buyin\n",
    "                self.positions.append(stock_price)\n",
    "                self.count.append(count)\n",
    "                self.Act1 += 1\n",
    "        elif act == 2:\n",
    "            if len(self.positions) > 0:\n",
    "                sell_ratio = self.determine_sell_ratio(amount)\n",
    "                num_positions_to_sell = int(len(self.positions) * sell_ratio)\n",
    "                for i in range(num_positions_to_sell):\n",
    "                    sell_price = self.data.iloc[self.t, :]['Close']\n",
    "                    buy_price = self.positions[i]\n",
    "                    count = self.count[i]\n",
    "                    abs_num = (sell_price - buy_price) * count\n",
    "                    if abs_num > 0:\n",
    "                        self.RW_p += 1\n",
    "                        self.RW_p_v += abs_num\n",
    "                    else:\n",
    "                        self.RW_n += 1\n",
    "                        self.RW_n_v += abs(abs_num)\n",
    "                    reward += sell_price * count\n",
    "                    self.profits += sell_price * count\n",
    "                self.positions = self.positions[num_positions_to_sell:]\n",
    "                self.count = self.count[num_positions_to_sell:]\n",
    "                self.Act2 += 1    \n",
    "        else:\n",
    "            self.Act0 += 1\n",
    "        \n",
    "        self.t += 1\n",
    "        if self.t >= len(self.data):\n",
    "            self.done = True\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['Close'] - p)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n",
    "        \n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward < 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history, reward, self.done\n",
    "    \n",
    "    def determine_sell_ratio(self, signal_strength):\n",
    "        thresholds = [0.2, 0.5, 0.8]\n",
    "        ratios = [0.25, 0.5, 0.75]\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if signal_strength < threshold:\n",
    "                return ratios[i]\n",
    "        return ratios[0]\n",
    "\n",
    "def train_dqn(env, Q, epoch_num=1):\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 1\n",
    "    confidence_threshold_buy = 0.11803540036662069\n",
    "    confidence_threshold_sell = 0.5689542195973996\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        RandAct = 0\n",
    "        NRandAct = 0\n",
    "        while not done and step < step_max:\n",
    "            pact = np.random.randint(3)\n",
    "            amount = 0.25\n",
    "            if np.random.rand() > epsilon:\n",
    "                q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                softmax_q_values = F.softmax(q_values).data\n",
    "                amount = softmax_q_values.ravel()[pact]\n",
    "                pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "                if pact == 2:\n",
    "                    if amount < confidence_threshold_sell:\n",
    "                        pact = 0\n",
    "                if pact == 1:\n",
    "                    if amount < confidence_threshold_buy:\n",
    "                        pact = 0\n",
    "                NRandAct+=1\n",
    "            else:\n",
    "                RandAct+=1\n",
    "                if pact==0: amount = 0.5\n",
    "            obs, reward, done = env.step(pact,amount)\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_profits = env.profits\n",
    "            if isinstance(env.count, list) and len(env.count) != 0:\n",
    "                for i in range(len(env.positions)):\n",
    "                    log_profits += env.count[i] * env.data.iloc[env.t, :]['Close']\n",
    "            else:\n",
    "                log_profits = env.profits\n",
    "            elapsed_time = time.time()-start\n",
    "            #print(f\"Training Epoch {epoch+1} - Profit: {log_profits}\")\n",
    "            start = time.time()       \n",
    "    return Q, total_losses, total_rewards\n",
    "\n",
    "def test_dqn(test_env, Q, confidence_threshold_buy=0.1, confidence_threshold_sell=0.8):\n",
    "    pobs = test_env.reset()\n",
    "    test_env.profits = 10000\n",
    "    test_acts = []\n",
    "    test_rewards = []\n",
    "    estimated_total_assets = [test_env.profits]\n",
    "\n",
    "    while not test_env.done:\n",
    "        q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        softmax_q_values = F.softmax(q_values).data\n",
    "        pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "        amount = softmax_q_values.ravel()[pact]\n",
    "        if pact == 2 and amount < confidence_threshold_sell:\n",
    "            pact = 0\n",
    "        elif pact == 1 and amount < confidence_threshold_buy:\n",
    "            pact = 0\n",
    "        obs, reward, done = test_env.step(pact, amount)\n",
    "        test_acts.append(pact)\n",
    "        test_rewards.append(reward)\n",
    "        pobs = obs\n",
    "        stock_value = sum(c * test_env.data.iloc[test_env.t]['Close'] for c in test_env.count)\n",
    "        total_assets = test_env.profits + stock_value\n",
    "        estimated_total_assets.append(total_assets)\n",
    "\n",
    "    final_profits = total_assets\n",
    "    return test_acts, test_rewards, final_profits\n",
    "\n",
    "def optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019):\n",
    "    # 資料分割\n",
    "    test_start_date_2010_2019 = start_date_2010 + datetime.timedelta(days=train_days_2010_2019)\n",
    "    train_end_date_2010_2019 = test_start_date_2010_2019 - datetime.timedelta(days=1)\n",
    "\n",
    "    train = data[start_date_2010.date():train_end_date_2010_2019.date()]\n",
    "    test = data[test_start_date_2010_2019.date():end_date_2019.date()]\n",
    "    train_env = Environment1(train)\n",
    "    test_env = Environment1(test)\n",
    "    \n",
    "    def objective(epoch_num):\n",
    "        Q = Q_Network(input_size=train_env.history_t + 1, hidden_size=100, output_size=3)\n",
    "        chainer.serializers.load_npz(f'Q_network_epoch_{int(epoch_num)}.npz', Q)\n",
    "        test_profits_list = []\n",
    "        for _ in range(100):\n",
    "            _, _, test_profits = test_dqn(test_env, Q)\n",
    "            test_profits_list.append(test_profits)\n",
    "        avg_test_profits = np.mean(test_profits_list)\n",
    "        return avg_test_profits\n",
    "    \n",
    "    pbounds = {\n",
    "        'epoch_num': (29, 299)\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "    \n",
    "    optimizer.maximize(\n",
    "        init_points=10,\n",
    "        n_iter=40,\n",
    "    )\n",
    "    \n",
    "    print(optimizer.max)\n",
    "\n",
    "data = pd.read_csv('C:/Users/User/Documents/RL_/Data/Stocks/SPY_Both.txt')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "start_date_2010 = datetime.datetime(2010, 1, 1)\n",
    "end_date_2019 = datetime.datetime(2019, 12, 31)\n",
    "total_days_2010_2019 = (end_date_2019 - start_date_2010).days\n",
    "train_days_2010_2019 = int(total_days_2010_2019 * 0.9)\n",
    "test_days_2010_2019 = total_days_2010_2019 - train_days_2010_2019\n",
    "\n",
    "optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2f2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | epoch_num |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m1.269e+04\u001b[0m | \u001b[0m141.6    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m1.275e+04\u001b[0m | \u001b[95m223.5    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m1.278e+04\u001b[0m | \u001b[95m29.03    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m110.6    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m68.62    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m1.267e+04\u001b[0m | \u001b[0m53.93    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m79.29    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m1.269e+04\u001b[0m | \u001b[0m122.3    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m136.1    \u001b[0m |\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m1.293e+04\u001b[0m | \u001b[95m174.5    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m1.274e+04\u001b[0m | \u001b[0m180.2    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m1.278e+04\u001b[0m | \u001b[0m173.8    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m1.274e+04\u001b[0m | \u001b[0m173.8    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m256.4    \u001b[0m |\n",
      "| \u001b[95m16       \u001b[0m | \u001b[95m1.294e+04\u001b[0m | \u001b[95m174.5    \u001b[0m |\n",
      "| \u001b[95m17       \u001b[0m | \u001b[95m1.294e+04\u001b[0m | \u001b[95m174.4    \u001b[0m |\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m1.295e+04\u001b[0m | \u001b[95m174.4    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m1.27e+04 \u001b[0m | \u001b[0m44.67    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m1.274e+04\u001b[0m | \u001b[0m173.9    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[95m23       \u001b[0m | \u001b[95m1.295e+04\u001b[0m | \u001b[95m174.2    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m198.3    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.2    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m1.289e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m1.291e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m1.295e+04\u001b[0m | \u001b[0m174.2    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m243.5    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m1.291e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m1.287e+04\u001b[0m | \u001b[0m165.8    \u001b[0m |\n",
      "| \u001b[95m39       \u001b[0m | \u001b[95m1.296e+04\u001b[0m | \u001b[95m174.4    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m174.2    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m1.261e+04\u001b[0m | \u001b[0m135.6    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m1.295e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m1.288e+04\u001b[0m | \u001b[0m174.2    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m1.29e+04 \u001b[0m | \u001b[0m174.3    \u001b[0m |\n",
      "| \u001b[95m47       \u001b[0m | \u001b[95m1.297e+04\u001b[0m | \u001b[95m174.4    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m1.29e+04 \u001b[0m | \u001b[0m174.2    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "=====================================\n",
      "{'target': 12971.720417955281, 'params': {'epoch_num': 174.39029520017382}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class Q_Network(chainer.Chain):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Q_Network, self).__init__(\n",
    "            fc1 = L.Linear(input_size, hidden_size),\n",
    "            fc2 = L.Linear(hidden_size, hidden_size),\n",
    "            fc3 = L.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "    def reset(self):\n",
    "        self.zerograds()\n",
    "\n",
    "class Environment1:\n",
    "    def __init__(self, data, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 10000\n",
    "        self.count = []\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.brok_rate = 0.0009\n",
    "        self.max_trade_percent = 0.8\n",
    "        self.tbrokerage = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        self.Act0 = 0\n",
    "        self.Act1 = 0\n",
    "        self.Act2 = 0\n",
    "        self.RW_p = 0\n",
    "        self.RW_n = 0\n",
    "        self.RW_p_v = 0\n",
    "        self.RW_n_v = 0\n",
    "        return [self.position_value] + self.history\n",
    "    \n",
    "    def step(self, act, amount):\n",
    "        if self.t >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "            return [self.position_value] + self.history, 0, self.done\n",
    "\n",
    "        reward = 0\n",
    "        if act == 1:\n",
    "            if self.profits != 0:\n",
    "                max_trade_amount = self.profits * self.max_trade_percent\n",
    "                stock_price = self.data.iloc[self.t, :]['Close']\n",
    "                count = max_trade_amount / stock_price\n",
    "                buyin = stock_price * count\n",
    "                self.profits -= buyin\n",
    "                self.positions.append(stock_price)\n",
    "                self.count.append(count)\n",
    "                self.Act1 += 1\n",
    "        elif act == 2:\n",
    "            if len(self.positions) > 0:\n",
    "                sell_ratio = self.determine_sell_ratio(amount)\n",
    "                num_positions_to_sell = int(len(self.positions) * sell_ratio)\n",
    "                for i in range(num_positions_to_sell):\n",
    "                    sell_price = self.data.iloc[self.t, :]['Close']\n",
    "                    buy_price = self.positions[i]\n",
    "                    count = self.count[i]\n",
    "                    abs_num = (sell_price - buy_price) * count\n",
    "                    if abs_num > 0:\n",
    "                        self.RW_p += 1\n",
    "                        self.RW_p_v += abs_num\n",
    "                    else:\n",
    "                        self.RW_n += 1\n",
    "                        self.RW_n_v += abs(abs_num)\n",
    "                    reward += sell_price * count\n",
    "                    self.profits += sell_price * count\n",
    "                self.positions = self.positions[num_positions_to_sell:]\n",
    "                self.count = self.count[num_positions_to_sell:]\n",
    "                self.Act2 += 1    \n",
    "        else:\n",
    "            self.Act0 += 1\n",
    "        \n",
    "        self.t += 1\n",
    "        if self.t >= len(self.data):\n",
    "            self.done = True\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['Close'] - p)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n",
    "        \n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward < 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history, reward, self.done\n",
    "    \n",
    "    def determine_sell_ratio(self, signal_strength):\n",
    "        thresholds = [0.2, 0.5, 0.8]\n",
    "        ratios = [0.25, 0.5, 0.75]\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if signal_strength < threshold:\n",
    "                return ratios[i]\n",
    "        return ratios[0]\n",
    "\n",
    "def train_dqn(env, Q, epoch_num=1):\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 1\n",
    "    confidence_threshold_buy = 0.11803540036662069\n",
    "    confidence_threshold_sell = 0.5689542195973996\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        RandAct = 0\n",
    "        NRandAct = 0\n",
    "        while not done and step < step_max:\n",
    "            pact = np.random.randint(3)\n",
    "            amount = 0.25\n",
    "            if np.random.rand() > epsilon:\n",
    "                q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                softmax_q_values = F.softmax(q_values).data\n",
    "                amount = softmax_q_values.ravel()[pact]\n",
    "                pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "                if pact == 2:\n",
    "                    if amount < confidence_threshold_sell:\n",
    "                        pact = 0\n",
    "                if pact == 1:\n",
    "                    if amount < confidence_threshold_buy:\n",
    "                        pact = 0\n",
    "                NRandAct+=1\n",
    "            else:\n",
    "                RandAct+=1\n",
    "                if pact==0: amount = 0.5\n",
    "            obs, reward, done = env.step(pact,amount)\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_profits = env.profits\n",
    "            if isinstance(env.count, list) and len(env.count) != 0:\n",
    "                for i in range(len(env.positions)):\n",
    "                    log_profits += env.count[i] * env.data.iloc[env.t, :]['Close']\n",
    "            else:\n",
    "                log_profits = env.profits\n",
    "            elapsed_time = time.time()-start\n",
    "            #print(f\"Training Epoch {epoch+1} - Profit: {log_profits}\")\n",
    "            start = time.time()       \n",
    "    return Q, total_losses, total_rewards\n",
    "\n",
    "def test_dqn(test_env, Q, confidence_threshold_buy=0.1, confidence_threshold_sell=0.8):\n",
    "    pobs = test_env.reset()\n",
    "    test_env.profits = 10000\n",
    "    test_acts = []\n",
    "    test_rewards = []\n",
    "    estimated_total_assets = [test_env.profits]\n",
    "\n",
    "    while not test_env.done:\n",
    "        q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        softmax_q_values = F.softmax(q_values).data\n",
    "        pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "        amount = softmax_q_values.ravel()[pact]\n",
    "        if pact == 2 and amount < confidence_threshold_sell:\n",
    "            pact = 0\n",
    "        elif pact == 1 and amount < confidence_threshold_buy:\n",
    "            pact = 0\n",
    "        obs, reward, done = test_env.step(pact, amount)\n",
    "        test_acts.append(pact)\n",
    "        test_rewards.append(reward)\n",
    "        pobs = obs\n",
    "        stock_value = sum(c * test_env.data.iloc[test_env.t]['Close'] for c in test_env.count)\n",
    "        total_assets = test_env.profits + stock_value\n",
    "        estimated_total_assets.append(total_assets)\n",
    "\n",
    "    final_profits = total_assets\n",
    "    return test_acts, test_rewards, final_profits\n",
    "\n",
    "def optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019):\n",
    "    # 資料分割\n",
    "    test_start_date_2010_2019 = start_date_2010 + datetime.timedelta(days=train_days_2010_2019)\n",
    "    train_end_date_2010_2019 = test_start_date_2010_2019 - datetime.timedelta(days=1)\n",
    "\n",
    "    train = data[start_date_2010.date():train_end_date_2010_2019.date()]\n",
    "    test = data[test_start_date_2010_2019.date():end_date_2019.date()]\n",
    "    train_env = Environment1(train)\n",
    "    test_env = Environment1(test)\n",
    "    \n",
    "    def objective(epoch_num):\n",
    "        Q = Q_Network(input_size=train_env.history_t + 1, hidden_size=100, output_size=3)\n",
    "        chainer.serializers.load_npz(f'Q_network_epoch_{int(epoch_num)}.npz', Q)\n",
    "        test_profits_list = []\n",
    "        for _ in range(100):\n",
    "            _, _, test_profits = test_dqn(test_env, Q)\n",
    "            test_profits_list.append(test_profits)\n",
    "        avg_test_profits = np.mean(test_profits_list)\n",
    "        return avg_test_profits\n",
    "    \n",
    "    pbounds = {\n",
    "        'epoch_num': (29, 299)\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "    \n",
    "    optimizer.maximize(\n",
    "        init_points=10,\n",
    "        n_iter=40,\n",
    "    )\n",
    "    \n",
    "    print(optimizer.max)\n",
    "\n",
    "data = pd.read_csv('C:/Users/User/Documents/RL_/Data/Stocks/SPY_Both.txt')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "start_date_2010 = datetime.datetime(2010, 1, 1)\n",
    "end_date_2019 = datetime.datetime(2019, 12, 31)\n",
    "total_days_2010_2019 = (end_date_2019 - start_date_2010).days\n",
    "train_days_2010_2019 = int(total_days_2010_2019 * 0.9)\n",
    "test_days_2010_2019 = total_days_2010_2019 - train_days_2010_2019\n",
    "\n",
    "optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177d5d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | epoch_num |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m1.266e+04\u001b[0m | \u001b[0m141.6    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m1.276e+04\u001b[0m | \u001b[95m223.5    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m1.276e+04\u001b[0m | \u001b[95m29.03    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m1.276e+04\u001b[0m | \u001b[0m110.6    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m1.277e+04\u001b[0m | \u001b[95m68.62    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m1.268e+04\u001b[0m | \u001b[0m53.93    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m79.29    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m1.266e+04\u001b[0m | \u001b[0m122.3    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m1.276e+04\u001b[0m | \u001b[0m136.1    \u001b[0m |\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m1.296e+04\u001b[0m | \u001b[95m174.5    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m1.28e+04 \u001b[0m | \u001b[0m181.2    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m167.4    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m177.0    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m1.272e+04\u001b[0m | \u001b[0m172.9    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m1.275e+04\u001b[0m | \u001b[0m175.3    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m173.9    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m1.293e+04\u001b[0m | \u001b[0m174.7    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m182.1    \u001b[0m |\n",
      "| \u001b[95m20       \u001b[0m | \u001b[95m1.297e+04\u001b[0m | \u001b[95m182.5    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m110.9    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m1.288e+04\u001b[0m | \u001b[0m183.0    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m1.277e+04\u001b[0m | \u001b[0m184.2    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m1.278e+04\u001b[0m | \u001b[0m113.9    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m1.263e+04\u001b[0m | \u001b[0m39.69    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m1.288e+04\u001b[0m | \u001b[0m182.4    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m1.264e+04\u001b[0m | \u001b[0m284.3    \u001b[0m |\n",
      "| \u001b[95m29       \u001b[0m | \u001b[95m1.3e+04  \u001b[0m | \u001b[95m182.6    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m1.299e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m1.291e+04\u001b[0m | \u001b[0m182.5    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m182.1    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m1.298e+04\u001b[0m | \u001b[0m182.1    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m182.6    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m174.4    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m1.295e+04\u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m1.295e+04\u001b[0m | \u001b[0m182.1    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m1.291e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m1.271e+04\u001b[0m | \u001b[0m40.77    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m1.296e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m1.297e+04\u001b[0m | \u001b[0m182.1    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m182.1    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m1.292e+04\u001b[0m | \u001b[0m182.5    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m1.294e+04\u001b[0m | \u001b[0m182.4    \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m1.289e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m1.297e+04\u001b[0m | \u001b[0m182.7    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m1.291e+04\u001b[0m | \u001b[0m182.5    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m1.291e+04\u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "=====================================\n",
      "{'target': 12997.429607014565, 'params': {'epoch_num': 182.55591798406144}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class Q_Network(chainer.Chain):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Q_Network, self).__init__(\n",
    "            fc1 = L.Linear(input_size, hidden_size),\n",
    "            fc2 = L.Linear(hidden_size, hidden_size),\n",
    "            fc3 = L.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y\n",
    "    def reset(self):\n",
    "        self.zerograds()\n",
    "\n",
    "class Environment1:\n",
    "    def __init__(self, data, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 10000\n",
    "        self.count = []\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.brok_rate = 0.0009\n",
    "        self.max_trade_percent = 0.8\n",
    "        self.tbrokerage = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        self.Act0 = 0\n",
    "        self.Act1 = 0\n",
    "        self.Act2 = 0\n",
    "        self.RW_p = 0\n",
    "        self.RW_n = 0\n",
    "        self.RW_p_v = 0\n",
    "        self.RW_n_v = 0\n",
    "        return [self.position_value] + self.history\n",
    "    \n",
    "    def step(self, act, amount):\n",
    "        if self.t >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "            return [self.position_value] + self.history, 0, self.done\n",
    "\n",
    "        reward = 0\n",
    "        if act == 1:\n",
    "            if self.profits != 0:\n",
    "                max_trade_amount = self.profits * self.max_trade_percent\n",
    "                stock_price = self.data.iloc[self.t, :]['Close']\n",
    "                count = max_trade_amount / stock_price\n",
    "                buyin = stock_price * count\n",
    "                self.profits -= buyin\n",
    "                self.positions.append(stock_price)\n",
    "                self.count.append(count)\n",
    "                self.Act1 += 1\n",
    "        elif act == 2:\n",
    "            if len(self.positions) > 0:\n",
    "                sell_ratio = self.determine_sell_ratio(amount)\n",
    "                num_positions_to_sell = int(len(self.positions) * sell_ratio)\n",
    "                for i in range(num_positions_to_sell):\n",
    "                    sell_price = self.data.iloc[self.t, :]['Close']\n",
    "                    buy_price = self.positions[i]\n",
    "                    count = self.count[i]\n",
    "                    abs_num = (sell_price - buy_price) * count\n",
    "                    if abs_num > 0:\n",
    "                        self.RW_p += 1\n",
    "                        self.RW_p_v += abs_num\n",
    "                    else:\n",
    "                        self.RW_n += 1\n",
    "                        self.RW_n_v += abs(abs_num)\n",
    "                    reward += sell_price * count\n",
    "                    self.profits += sell_price * count\n",
    "                self.positions = self.positions[num_positions_to_sell:]\n",
    "                self.count = self.count[num_positions_to_sell:]\n",
    "                self.Act2 += 1    \n",
    "        else:\n",
    "            self.Act0 += 1\n",
    "        \n",
    "        self.t += 1\n",
    "        if self.t >= len(self.data):\n",
    "            self.done = True\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['Close'] - p)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n",
    "        \n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward < 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history, reward, self.done\n",
    "    \n",
    "    def determine_sell_ratio(self, signal_strength):\n",
    "        thresholds = [0.2, 0.5, 0.8]\n",
    "        ratios = [0.25, 0.5, 0.75]\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if signal_strength < threshold:\n",
    "                return ratios[i]\n",
    "        return ratios[0]\n",
    "\n",
    "def train_dqn(env, Q, epoch_num=1):\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 1\n",
    "    confidence_threshold_buy = 0.11803540036662069\n",
    "    confidence_threshold_sell = 0.5689542195973996\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        RandAct = 0\n",
    "        NRandAct = 0\n",
    "        while not done and step < step_max:\n",
    "            pact = np.random.randint(3)\n",
    "            amount = 0.25\n",
    "            if np.random.rand() > epsilon:\n",
    "                q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                softmax_q_values = F.softmax(q_values).data\n",
    "                amount = softmax_q_values.ravel()[pact]\n",
    "                pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "                if pact == 2:\n",
    "                    if amount < confidence_threshold_sell:\n",
    "                        pact = 0\n",
    "                if pact == 1:\n",
    "                    if amount < confidence_threshold_buy:\n",
    "                        pact = 0\n",
    "                NRandAct+=1\n",
    "            else:\n",
    "                RandAct+=1\n",
    "                if pact==0: amount = 0.5\n",
    "            obs, reward, done = env.step(pact,amount)\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_profits = env.profits\n",
    "            if isinstance(env.count, list) and len(env.count) != 0:\n",
    "                for i in range(len(env.positions)):\n",
    "                    log_profits += env.count[i] * env.data.iloc[env.t, :]['Close']\n",
    "            else:\n",
    "                log_profits = env.profits\n",
    "            elapsed_time = time.time()-start\n",
    "            #print(f\"Training Epoch {epoch+1} - Profit: {log_profits}\")\n",
    "            start = time.time()       \n",
    "    return Q, total_losses, total_rewards\n",
    "\n",
    "def test_dqn(test_env, Q, confidence_threshold_buy=0.1, confidence_threshold_sell=0.8):\n",
    "    pobs = test_env.reset()\n",
    "    test_env.profits = 10000\n",
    "    test_acts = []\n",
    "    test_rewards = []\n",
    "    estimated_total_assets = [test_env.profits]\n",
    "\n",
    "    while not test_env.done:\n",
    "        q_values = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "        softmax_q_values = F.softmax(q_values).data\n",
    "        pact = np.random.choice(3, p=softmax_q_values.ravel())\n",
    "        amount = softmax_q_values.ravel()[pact]\n",
    "        if pact == 2 and amount < confidence_threshold_sell:\n",
    "            pact = 0\n",
    "        elif pact == 1 and amount < confidence_threshold_buy:\n",
    "            pact = 0\n",
    "        obs, reward, done = test_env.step(pact, amount)\n",
    "        test_acts.append(pact)\n",
    "        test_rewards.append(reward)\n",
    "        pobs = obs\n",
    "        stock_value = sum(c * test_env.data.iloc[test_env.t]['Close'] for c in test_env.count)\n",
    "        total_assets = test_env.profits + stock_value\n",
    "        estimated_total_assets.append(total_assets)\n",
    "\n",
    "    final_profits = total_assets\n",
    "    return test_acts, test_rewards, final_profits\n",
    "\n",
    "def optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019):\n",
    "    # 資料分割\n",
    "    test_start_date_2010_2019 = start_date_2010 + datetime.timedelta(days=train_days_2010_2019)\n",
    "    train_end_date_2010_2019 = test_start_date_2010_2019 - datetime.timedelta(days=1)\n",
    "\n",
    "    train = data[start_date_2010.date():train_end_date_2010_2019.date()]\n",
    "    test = data[test_start_date_2010_2019.date():end_date_2019.date()]\n",
    "    train_env = Environment1(train)\n",
    "    test_env = Environment1(test)\n",
    "    \n",
    "    def objective(epoch_num):\n",
    "        Q = Q_Network(input_size=train_env.history_t + 1, hidden_size=100, output_size=3)\n",
    "        chainer.serializers.load_npz(f'Q_network_epoch_{int(epoch_num)}.npz', Q)\n",
    "        test_profits_list = []\n",
    "        for _ in range(100):\n",
    "            _, _, test_profits = test_dqn(test_env, Q)\n",
    "            test_profits_list.append(test_profits)\n",
    "        avg_test_profits = np.mean(test_profits_list)\n",
    "        return avg_test_profits\n",
    "    \n",
    "    pbounds = {\n",
    "        'epoch_num': (29, 299)\n",
    "    }\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "    )\n",
    "    \n",
    "    optimizer.maximize(\n",
    "        init_points=10,\n",
    "        n_iter=40,\n",
    "    )\n",
    "    \n",
    "    print(optimizer.max)\n",
    "\n",
    "data = pd.read_csv('C:/Users/User/Documents/RL_/Data/Stocks/SPY_Both.txt')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "\n",
    "start_date_2010 = datetime.datetime(2010, 1, 1)\n",
    "end_date_2019 = datetime.datetime(2019, 12, 31)\n",
    "total_days_2010_2019 = (end_date_2019 - start_date_2010).days\n",
    "train_days_2010_2019 = int(total_days_2010_2019 * 0.9)\n",
    "test_days_2010_2019 = total_days_2010_2019 - train_days_2010_2019\n",
    "\n",
    "optimize_hyperparameters(data, start_date_2010, end_date_2019, train_days_2010_2019, test_days_2010_2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b95fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
